{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UA4TwsBYp_1u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import math\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import google.generativeai as genai\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
        "from peft import LoraConfig, PeftModel\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset, Dataset, DatasetDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = userdata.get('OPENAI_API_KEY').strip()\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key"
      ],
      "metadata": {
        "id": "QPK1l7dGqwJu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if openai_api_key:\n",
        "  print(\"OpenAI API key is set\")\n",
        "else:\n",
        "  print(\"OpenAI API key is not set\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVNMQtJsBcWB",
        "outputId": "cf4fefa6-01e3-4fd8-f55a-bc4ccf2182b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key is set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysntT-y7BoQC",
        "outputId": "71540320-e577-4709-d865-7575d8caca18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-01 09:08:57--  https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 173646 (170K) [text/plain]\n",
            "Saving to: ‘mini-llama-articles.csv’\n",
            "\n",
            "mini-llama-articles 100%[===================>] 169.58K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-10-01 09:08:57 (3.12 MB/s) - ‘mini-llama-articles.csv’ saved [173646/173646]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_chunks(text,chunk_size=1024):\n",
        "    chunks = []\n",
        "    for i in range(0,len(text),chunk_size):\n",
        "        chunks.append(text[i:i+chunk_size])\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "-gKhP0njB1dH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = []\n",
        "with open(\"./mini-llama-articles.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
        "  csv_reader = csv.reader(file)\n",
        "  for idx, row in enumerate( csv_reader ):\n",
        "    if idx ==0: continue;\n",
        "    chunks.extend(split_into_chunks(row[1]))\n",
        "\n",
        "print(\"number of articles:\", idx)\n",
        "print(\"number of chunks:\", len(chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM-cmIS9DC-J",
        "outputId": "fa43725d-72ab-4c05-d66a-6f4a35f1c097"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of articles: 14\n",
            "number of chunks: 174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(chunks,columns=['chunk'])\n",
        "print(df.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMQ1FzL8EAHC",
        "outputId": "2198ae15-f273-40f5-f8d7-9dd2b05bdb6d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['chunk'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "JIAHDhbTEadB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text):\n",
        "  try:\n",
        "    text =text.replace(\"\\n\", \" \")\n",
        "    res =client.embeddings.create(input=[text], model=\"text-embedding-3-small\")\n",
        "    return res.data[0].embedding\n",
        "  except:\n",
        "    return"
      ],
      "metadata": {
        "id": "JSWyfNeIEvdo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings =[]\n",
        "for index, row in tqdm(df.iterrows()):\n",
        "  embeddings.append(get_embedding(row['chunk']))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MalK0611FO1J",
        "outputId": "61746a03-ed19-4825-99c0-52582e20b648"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "174it [00:53,  3.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding column to the dataframe\n",
        "embeddings_values = pd.Series(embeddings)\n",
        "df.insert(loc=1,column='embedding',value=embeddings_values)"
      ],
      "metadata": {
        "id": "IC8EGr2aF60W"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION =\"How many parameters LLaMA2 model has?\"\n",
        "QUESTION_emb =get_embedding(QUESTION)\n",
        "BAD_SOURCE_emb =get_embedding(\"The sky is blue.\")\n",
        "GOOD_SOURCE_emb =get_embedding(\"LLaMA2 model has a total of 2B parameters.\")"
      ],
      "metadata": {
        "id": "fp_bTxv1Gdfr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(QUESTION_emb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTn8bFDsHwKN",
        "outputId": "d7ebdb27-abca-48fd-b6c1-b9530c03386d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparison of cosine similarity\n",
        "print(\"> Bad Response Score: \",cosine_similarity([QUESTION_emb],[BAD_SOURCE_emb]))\n",
        "print(\"> Good Response Score:\", cosine_similarity([QUESTION_emb], [GOOD_SOURCE_emb]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzB-765IH1VI",
        "outputId": "01d1df96-bc47-401b-a0b4-f5ab61a5da43"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Bad Response Score:  [[0.02578727]]\n",
            "> Good Response Score: [[0.83154609]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION =\"How many parameters LLaMA2 model has?\"\n",
        "QUESTION_emb =get_embedding(QUESTION)\n",
        "cosine_similarities =cosine_similarity([QUESTION_emb], df['embedding'].tolist())\n",
        "print(cosine_similarities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy2unH-5Iu8X",
        "outputId": "1023c447-1499-42c8-d501-a09a55772218"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.46767499 0.46912464 0.25982343 0.29393922 0.319654   0.40157167\n",
            "  0.41500898 0.4525136  0.45935869 0.1259955  0.11750504 0.01348838\n",
            "  0.22602134 0.21423916 0.10145219 0.33064027 0.1074138  0.34682608\n",
            "  0.16311555 0.08726645 0.3482437  0.22839007 0.19203919 0.26471736\n",
            "  0.24928956 0.34824073 0.24828999 0.32761311 0.41416043 0.41337977\n",
            "  0.46363194 0.38341214 0.46851769 0.35636739 0.35398223 0.3027117\n",
            "  0.29929401 0.29252605 0.40035147 0.4646832  0.39473083 0.41042047\n",
            "  0.4470362  0.43173664 0.35909244 0.33965997 0.51344046 0.20932135\n",
            "  0.40206751 0.32829097 0.42863159 0.48270619 0.45036044 0.34256287\n",
            "  0.32083244 0.42588004 0.24622426 0.18089188 0.23648678 0.34271678\n",
            "  0.3437286  0.20476358 0.19768159 0.22446578 0.21110849 0.42281591\n",
            "  0.26382997 0.30438172 0.33609101 0.38368357 0.23536253 0.24351588\n",
            "  0.37074498 0.28025883 0.49052816 0.53044055 0.37853176 0.43770825\n",
            "  0.37750013 0.39255233 0.30081934 0.41710617 0.4674553  0.45420047\n",
            "  0.35169137 0.21222866 0.4262131  0.31603804 0.44059904 0.52726652\n",
            "  0.50597773 0.49750504 0.44281669 0.35109171 0.39478983 0.44131221\n",
            "  0.20327756 0.27925495 0.15406605 0.19138923 0.1591823  0.24050776\n",
            "  0.22525528 0.19943315 0.26228114 0.35059169 0.36222266 0.15316145\n",
            "  0.27644707 0.45339784 0.3342825  0.2945309  0.381719   0.41724067\n",
            "  0.61951453 0.3868869  0.3444073  0.2827669  0.20121774 0.14609842\n",
            "  0.19517217 0.28226397 0.15626344 0.18060334 0.30277618 0.28139129\n",
            "  0.30262487 0.23778126 0.14555983 0.19755626 0.39237552 0.33007145\n",
            "  0.23546332 0.1570049  0.26884917 0.26237333 0.37819181 0.18126\n",
            "  0.13054523 0.18452304 0.26052189 0.35587602 0.33354572 0.23507334\n",
            "  0.37106625 0.19023676 0.18962526 0.20080808 0.16389302 0.3500477\n",
            "  0.25243468 0.33878725 0.18292758 0.30649191 0.24205178 0.13082281\n",
            "  0.18217836 0.19085239 0.41308701 0.16363392 0.26363609 0.20631095\n",
            "  0.30178322 0.24799972 0.41006581 0.21783605 0.22282313 0.27780672\n",
            "  0.14569464 0.19755302 0.35371121 0.1539897  0.32274227 0.30311173]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_chunks_to_retrieve = 3\n",
        "indices =np.argsort(cosine_similarities[0])[::-1][:number_of_chunks_to_retrieve]\n",
        "print(indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NCodChxJC83",
        "outputId": "404af5eb-6068-40f8-a817-27463cf1fdad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[114  75  89]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, item in enumerate(df.chunk[indices]):\n",
        "  print(f\"> Chunk {idx+1}\")\n",
        "  print(item)\n",
        "  print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0fFdmOEJXoS",
        "outputId": "944e56b5-9d03-4ed5-f2e1-4c4637e4bffb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Chunk 1\n",
            "by Meta that ventures into both the AI and academic spaces. The model aims to help researchers, scientists, and engineers advance their work in exploring AI applications. It will be released under a non-commercial license to prevent misuse, and access will be granted to academic researchers, individuals, and organizations affiliated with the government, civil society, academia, and industry research facilities on a selective case-by-case basis. The sharing of codes and weights allows other researchers to test new approaches in LLMs. The LLaMA models have a range of 7 billion to 65 billion parameters. LLaMA-65B can be compared to DeepMind's Chinchilla and Google's PaLM. Publicly available unlabeled data was used to train these models, and training smaller foundational models require less computing power and resources. LLaMA 65B and 33B have been trained on 1.4 trillion tokens in 20 different languages, and according to the Facebook Artificial Intelligence Research (FAIR) team, the model's performance varies ac\n",
            "----\n",
            "> Chunk 2\n",
            "LLaMA: Meta's new AI tool According to the official release, LLaMA is a foundational language model developed to assist 'researchers and academics' in their work (as opposed to the average web user) to understand and study these NLP models. Leveraging AI in such a way could give researchers an edge in terms of time spent. You may not know this, but this would be Meta's third LLM after Blender Bot 3 and Galactica. However, the two LLMs were shut down soon, and Meta stopped their further development, as it produced erroneous results. Before moving further, it is important to emphasize that LLaMA is NOT a chatbot like ChatGPT. As I mentioned before, it is a 'research tool' for researchers. We can expect the initial versions of LLaMA to be a bit more technical and indirect to use as opposed to the case with ChatGPT, which was very direct, interactive, and a lot easy to use. \"Smaller, more performant models such as LLaMA enable ... research community who don't have access to large amounts of infrastructure to stud\n",
            "----\n",
            "> Chunk 3\n",
            "I. Llama 2: Revolutionizing Commercial Use Unlike its predecessor Llama 1, which was limited to research use, Llama 2 represents a major advancement as an open-source commercial model. Businesses can now integrate Llama 2 into products to create AI-powered applications. Availability on Azure and AWS facilitates fine-tuning and adoption. However, restrictions apply to prevent exploitation. Companies with over 700 million active daily users cannot use Llama 2. Additionally, its output cannot be used to improve other language models.  II. Llama 2 Model Flavors Llama 2 is available in four different model sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters. While 7B, 13B, and 70B have already been released, the 34B model is still awaited. The pretrained variant, trained on a whopping 2 trillion tokens, boasts a context window of 4096 tokens, twice the size of its predecessor Llama 1. Meta also released a Llama 2 fine-tuned model for chat applications that was trained on over 1 million human annota\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  system_prompt = (\n",
        "      \"You are an assistant and expert in answering questions from a chunks of content. \"\n",
        "      \"Only answer AI-related question, else say that you cannot answer this question.\")\n",
        "  prompt = (\n",
        "      \"Read the following informations that might contain the context you require to answer the question. You can use the informations starting from the <START_OF_CONTEXT> tag and end with the <END_OF_CONTEXT> tag. Here is the content:\\n\\n<START_OF_CONTEXT>\\n{}\\n<END_OF_CONTEXT>\\n\\n\"\n",
        "      \"Please provide an informative and accurate answer to the following question based on the avaiable context. Be concise and take your time. \\nQuestion: {}\\nAnswer:\"\n",
        "  )\n",
        "  # Adding the retrieves pieces of text to our prompt\n",
        "  prompt = prompt.format(\"\".join(df.chunk[indices]),QUESTION)\n",
        "  #print(prompt)\n",
        "  #model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\",system_instruction=system_prompt)\n",
        "  #result = model.generate_content(prompt,request_options={\"timeout\": 1000})\n",
        "  #res = result.text\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"gpt-4o-mini\",\n",
        "      messages = [\n",
        "          {\"role\":\"system\",\"content\":system_prompt},\n",
        "          {\"role\":\"user\",\"content\":prompt}\n",
        "      ],\n",
        "      max_tokens = 500,\n",
        "      temperature=0.7\n",
        "  )\n",
        "  res = response.choices[0].message.content\n",
        "  print(res)\n",
        "except Exception as e:\n",
        "  print(f\"An error has occured: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBo_9LPYJhHq",
        "outputId": "855be233-23ac-4f0b-b5b2-5682da5522a2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLaMA 2 model has four different sizes with the following parameters: 7 billion, 13 billion, 34 billion, and 70 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WtQo3-JTNYa2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}